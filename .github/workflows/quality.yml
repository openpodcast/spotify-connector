name: Code Quality

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Check out repository code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Needed for SonarCloud

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov coverage black isort flake8 pylint mypy
        pip install -e .

    - name: Install package dependencies
      run: |
        if [ -f "Pipfile" ]; then
          pip install pipenv
          pipenv requirements > requirements.txt
          pip install -r requirements.txt
        fi
      continue-on-error: true

    - name: Run tests with coverage
      run: |
        python -m pytest tests/ --cov=spotifyconnector --cov-report=xml --cov-report=html --cov-report=term-missing

    - name: Generate coverage badge
      run: |
        coverage-badge -o coverage.svg

    - name: Run type checking with mypy
      run: |
        mypy spotifyconnector/ --ignore-missing-imports || true

    - name: Check code complexity with radon
      run: |
        pip install radon
        radon cc spotifyconnector/ -a -s || true
        radon mi spotifyconnector/ -s || true

    - name: Run code quality analysis
      run: |
        # Generate quality metrics
        echo "## Code Quality Report" > quality-report.md
        echo "" >> quality-report.md
        
        echo "### Test Coverage" >> quality-report.md
        coverage report --format=markdown >> quality-report.md || true
        
        echo "" >> quality-report.md
        echo "### Code Complexity" >> quality-report.md
        radon cc spotifyconnector/ -a -s >> quality-report.md || true

    - name: Upload coverage reports
      uses: actions/upload-artifact@v4
      with:
        name: coverage-reports
        path: |
          coverage.xml
          htmlcov/
          coverage.svg
          quality-report.md

    - name: SonarCloud Scan
      uses: SonarSource/sonarcloud-github-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
      continue-on-error: true

  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Check out repository code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark memory-profiler
        pip install -e .

    - name: Run performance tests
      run: |
        # Create a simple performance test
        cat > test_performance.py << 'EOF'
        import pytest
        from spotifyconnector.connector import SpotifyConnector
        from unittest.mock import patch, Mock
        
        @pytest.mark.benchmark
        def test_connector_creation_performance(benchmark):
            """Test SpotifyConnector creation performance."""
            def create_connector():
                return SpotifyConnector(
                    base_url="https://test.com",
                    client_id="test",
                    podcast_id="test",
                    sp_dc="test",
                    sp_key="test"
                )
            result = benchmark(create_connector)
            assert result is not None
        
        @pytest.mark.benchmark 
        def test_url_building_performance(benchmark):
            """Test URL building performance."""
            connector = SpotifyConnector(
                base_url="https://test.com",
                client_id="test", 
                podcast_id="test",
                sp_dc="test",
                sp_key="test"
            )
            result = benchmark(connector._build_url, "shows", "123", "aggregate")
            assert "shows/123/aggregate" in result
        EOF
        
        python -m pytest test_performance.py --benchmark-only --benchmark-json=benchmark.json || true

    - name: Upload performance results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: benchmark.json
